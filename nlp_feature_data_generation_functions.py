# -*- coding: utf-8 -*-
"""NLP Feature Data Generation Functions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jKNjDHk6dABTBewIazroziHes-IBgOUC

Note: this python file contains the NLP fine tuning function FT_NLP(), the and the NLP feature extraction function NLPF(), and the batched NLP feature extraction function NLPF_batching(). The functions contained in this file should be called from the Testing Notebook.ipynb file. Examples of using these functions are provided in the MLP Testing Notebook.ipynb file.

Note: any code used from a source has been reference via a comment directly next to that specific line of code. A majority of the pretraining functionality and NLP feature extraction is using a guide from Hugging Face that is referenced directly in the functions below.
"""

#from google.colab import drive
#drive.mount('/content/drive')

import numpy as np
import pandas as pd

#!pip install torch-summary 
#!pip install datasets 
#!pip install transformers

#this function is called to fine tune the Hugging Face Distilbert model with training data from the provided sample dataset

def FT_NLP(data_list,label_list):

  from sklearn.model_selection import train_test_split
  import numpy as np
  import pandas as pd
  import torch
  import transformers as ppb #https://huggingface.co/docs/transformers/tasks/sequence_classification
  import warnings #https://huggingface.co/docs/transformers/tasks/sequence_classification
  warnings.filterwarnings('ignore') #https://huggingface.co/docs/transformers/tasks/sequence_classification
  from datasets import Dataset #https://huggingface.co/docs/transformers/tasks/sequence_classification
  import datasets #https://huggingface.co/docs/transformers/tasks/sequence_classification

  #extract just the Info field and labels from the dataset
  data_df = data_list['Info']
  labels = label_list['label']

  #split the dataset into train/test for the purpose of creating a data dictionary to fine tune the NLP model
  X_train, X_test, y_train, y_test = train_test_split(data_df, labels, test_size=0.3, random_state=42)

  #train portion of the data dictionary
  train_df = pd.DataFrame({ #https://stackoverflow.com/questions/71618974/convert-pandas-dataframe-to-datasetdict
      "label" : y_train, #https://stackoverflow.com/questions/71618974/convert-pandas-dataframe-to-datasetdict
      "text" : X_train #https://stackoverflow.com/questions/71618974/convert-pandas-dataframe-to-datasetdict
  }) 

  #test portion of the data dictionary
  test_df = pd.DataFrame({ #https://stackoverflow.com/questions/71618974/convert-pandas-dataframe-to-datasetdict
      "label" : y_test, #https://stackoverflow.com/questions/71618974/convert-pandas-dataframe-to-datasetdict
      "text" : X_test #https://stackoverflow.com/questions/71618974/convert-pandas-dataframe-to-datasetdict
  })

  #create the data dictionary using both the train and test portion
  train_dataset = Dataset.from_dict(train_df) #https://stackoverflow.com/questions/71618974/convert-pandas-dataframe-to-datasetdict
  test_dataset = Dataset.from_dict(test_df) #https://stackoverflow.com/questions/71618974/convert-pandas-dataframe-to-datasetdict
  my_dataset_dict = datasets.DatasetDict({"train":train_dataset,"test":test_dataset}) #https://stackoverflow.com/questions/71618974/convert-pandas-dataframe-to-datasetdict

  #load Distilbert tokenizer from Hugging Face
  from transformers import AutoTokenizer #https://huggingface.co/docs/transformers/tasks/sequence_classification
  tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased") #https://huggingface.co/docs/transformers/tasks/sequence_classification

  def preprocess_function(examples): #https://huggingface.co/docs/transformers/tasks/sequence_classification
    return tokenizer(examples["text"], truncation=True) #https://huggingface.co/docs/transformers/tasks/sequence_classification

  #tokenize the Info strings from the newly created data dictionary
  tokenized = my_dataset_dict.map(preprocess_function, batched=True) #https://huggingface.co/docs/transformers/tasks/sequence_classification

  #load model for fine tuning
  from transformers import DataCollatorWithPadding #https://huggingface.co/docs/transformers/tasks/sequence_classification
  data_collator = DataCollatorWithPadding(tokenizer=tokenizer) #https://huggingface.co/docs/transformers/tasks/sequence_classification

  from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer #https://huggingface.co/docs/transformers/tasks/sequence_classification
  model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased", num_labels=2) #https://huggingface.co/docs/transformers/tasks/sequence_classification

  training_args = TrainingArguments( #https://huggingface.co/docs/transformers/tasks/sequence_classification
    output_dir="./results", #https://huggingface.co/docs/transformers/tasks/sequence_classification
    learning_rate=2e-5, #https://huggingface.co/docs/transformers/tasks/sequence_classification
    per_device_train_batch_size=15, #https://huggingface.co/docs/transformers/tasks/sequence_classification
    per_device_eval_batch_size=15, #https://huggingface.co/docs/transformers/tasks/sequence_classification
    num_train_epochs=1, #https://huggingface.co/docs/transformers/tasks/sequence_classification
    weight_decay=0.01, #https://huggingface.co/docs/transformers/tasks/sequence_classification
  )

  trainer = Trainer( #https://huggingface.co/docs/transformers/tasks/sequence_classification
    model=model, #https://huggingface.co/docs/transformers/tasks/sequence_classification
    args=training_args, #https://huggingface.co/docs/transformers/tasks/sequence_classification
    train_dataset=tokenized["train"], #https://huggingface.co/docs/transformers/tasks/sequence_classification
    eval_dataset=tokenized["test"], #https://huggingface.co/docs/transformers/tasks/sequence_classification
    tokenizer=tokenizer, #https://huggingface.co/docs/transformers/tasks/sequence_classification
    data_collator=data_collator, #https://huggingface.co/docs/transformers/tasks/sequence_classification
  )

  #fine tune Hugging Face Distilbert model:
  trainer.train() #https://huggingface.co/docs/transformers/tasks/sequence_classification

  return model

#model = FT_NLP(data_df,labels)

# this function uses the newly fine tuned Hugging Face Distilbert model to generate sentence embeddings from the provided sample dataset

def NLP_Features(data_list,label_list,model):

  from sklearn.model_selection import train_test_split
  import numpy as np
  import pandas as pd
  import torch
  import transformers as ppb
  import warnings
  warnings.filterwarnings('ignore')
  from datasets import Dataset
  import datasets

  #load just the Info field and labels from the sample dataset
  data_df = data_list['Info']
  labels = label_list['label']

  a = data_df.size

  #load Distilber tokenizer:
  model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')#https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb
  tokenizer = tokenizer_class.from_pretrained(pretrained_weights) #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb
  #model = model_class.from_pretrained(pretrained_weights) #not going to use the pretained weights, as we have just fine tuned the model #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb

  done = 0 #done flag
  train_NLP = np.empty((0,768), int) #create an empty array to put the newly generated sentence embedding for each sample

  #this for loop is needed, as the sample sizes used in this project are often too large (i.e. 100k samples) to run all at once. Do not have enough ran with colab to run.
  #instead, the sentence embeddings are generated in batches - for example, groups of 150.
  for i in range(0,1000):

    if done == 1:
      break
    
    #checking how many samples are left to generate. If more than 150, run full 150. If less, run that amount
    if i*150 <= a-150:
      start_index = i*150
      end_index = (i+1)*150

    if i*150 > a-150:
      done = 1
      start_index = i*150
      end_index = a

    #isolate batch of samples that are going to have sentence embeddings generated this batch
    batch = data_df[start_index:end_index]

    #tokenize sample strings
    tokenized = batch.apply((lambda x: tokenizer.encode(x, add_special_tokens=True))) # #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb

    #padding:
    max_len = 0 #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb
    for i in tokenized.values: #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb
        if len(i) > max_len: #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb
            max_len = len(i) #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb
    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values]) #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb

    #have model ignore the masks
    attention_mask = np.where(padded != 0, 1, 0) #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb
    attention_mask.shape #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb

    #create torch tensors from both input ids and masks. both are required inputs for Distilbert
    input_ids = torch.tensor(padded) #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb  
    attention_mask = torch.tensor(attention_mask) #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb

    #limite tokens to 512, the Distilbert limit
    input_ids = input_ids[:,0:512]
    attention_mask = attention_mask[:,0:512]

    #run model
    with torch.no_grad(): #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb
      last_hidden_states = model.distilbert(input_ids, attention_mask=attention_mask) #https://colab.research.google.com/github/jalammar/jalammar.github.io/blob/master/notebooks/bert/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb

    #extract features from first position in each sentence embedding provided by Distilbert
    features = last_hidden_states[0][:,0,:].numpy()

    #add these new sentence embeddings to the matrix that will output
    train_NLP = np.append(train_NLP, features, axis=0)

    print(np.shape(train_NLP))
  
  return train_NLP



#NLP_features = NLP_Features(data_df,labels)

#np.shape(NLP_features)

#NLP_features = pd.DataFrame(NLP_features)

#NLP_features_for_batching = pd.concat([data_df,NLP_features,labels], axis = 1)

#NLP_features_for_batching=NLP_features_for_batching.drop(columns=['No'])

#this function takes the generated sentence embeddings from Distilbert and averages them in groups of 10 based on IP address

def NLP_batching(data,batch_size,a):
  #next we will batch these features based on IP address:
  batching_samples = data.head(a) #https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html
  batching_samples = batching_samples.to_numpy() 

  start_index = 0 #start at the first IP address
  stop_index = 0
  #new_data = np.zeros((a,17)) # new matrix to contain our newly batched features
  new_data = np.empty((0,769), int)
  done = 0

  #loop through all of the newly genenerated sentence embeddings
  for j in range (0,a):

    if done == 1:
      break

    next_feature = np.zeros((1,768)) # blank array to hold the next set of features generated from each loop


    ip_start = batching_samples[start_index,1] # locate the IP address for the starting index

  #look at the next 10 samples, decide where to stop(either at 10, or when the IP address changes). If we reach the end of all samples, then stop there
    if start_index >= a-batch_size:
      for i in range (start_index+1,a):
        if ip_start != batching_samples[i,1]:
          stop_index = i
          done = 1
          break
        stop_index = i
        done = 1
    else:
      for i in range (start_index+1,start_index+batch_size):
        if ip_start != batching_samples[i,1]:
          stop_index = i
          break
        stop_index = i

    #matrix that contains only the rows we care about for this specific batch
    batch_matrix = batching_samples[start_index:stop_index,:]

    #calculate final attack label
    attack_sum = sum(batch_matrix[:,774])
    if attack_sum > 0 :
      attack = np.ones((1,1)) 
    else:
      attack = np.zeros((1,1)) 
    
    #average the 768 sentence embeddings accross the 10 batched features
    next_feature[0:768] = np.mean(batch_matrix[:,6:774],axis=0)
    next_feature = np.concatenate([next_feature,attack],axis=1)

    #append new feature set with this loops newly generated feature
    new_data = np.append(new_data,next_feature,axis=0)

    start_index = stop_index

  return new_data

#NLP_batched_features = NLP_batching(NLP_features_for_batching,10,500)

#np.shape(NLP_batched_features)

